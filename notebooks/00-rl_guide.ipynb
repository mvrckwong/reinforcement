{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reinforcement Learning with Ray RLlib - Comprehensive Guide\n",
        "\n",
        "This notebook provides a detailed guide to configuring and training RL algorithms with comprehensive hyperparameter control.\n",
        "\n",
        "## Key Hyperparameter Categories\n",
        "\n",
        "### 1. Training Hyperparameters\n",
        "- **gamma (Î³)**: Discount factor (0-1). How much to value future rewards vs immediate rewards\n",
        "- **lr**: Learning rate. Step size for gradient descent updates\n",
        "- **train_batch_size**: Total samples used per training iteration\n",
        "- **entropy_coeff**: Encourages exploration by penalizing deterministic policies\n",
        "- **vf_loss_coeff**: Weight of value function loss in total loss\n",
        "\n",
        "### 2. Model Architecture\n",
        "- **fcnet_hiddens**: List of hidden layer sizes (e.g., [256, 256] = 2 layers, 256 units each)\n",
        "- **fcnet_activation**: Activation function (relu, tanh, elu, etc.)\n",
        "- **vf_share_layers**: Whether policy and value networks share parameters\n",
        "\n",
        "### 3. Environment Runners\n",
        "- **num_env_runners**: Parallel workers collecting experience\n",
        "- **rollout_fragment_length**: Steps collected before sending to learner\n",
        "\n",
        "### 4. Algorithm-Specific\n",
        "- **PPO**: Uses clipped surrogate objective with `clip_param` for stable updates\n",
        "- **IMPALA**: Uses V-trace for off-policy correction with importance sampling\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PPO (Proximal Policy Optimization)\n",
        "\n",
        "PPO is an on-policy algorithm that uses a clipped surrogate objective to prevent large policy updates.\n",
        "\n",
        "**Key features:**\n",
        "- On-policy: Uses recently collected data\n",
        "- Stable training through policy clipping\n",
        "- Good for continuous and discrete action spaces\n",
        "\n",
        "**Algorithm Overview:**\n",
        "1. Collect trajectories using current policy\n",
        "2. Compute advantages using value function\n",
        "3. Update policy using clipped objective\n",
        "4. Update value function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "\n",
        "# Create and configure PPO with detailed hyperparameters\n",
        "config = (\n",
        "    PPOConfig()\n",
        "    .environment(\"CartPole-v1\")\n",
        "    \n",
        "    # Training hyperparameters\n",
        "    .training(\n",
        "        gamma=0.99,                      # Discount factor for future rewards\n",
        "        lr=5e-5,                         # Learning rate for optimizer\n",
        "        train_batch_size=4000,           # Total batch size for training\n",
        "        sgd_minibatch_size=128,          # Size of minibatches for SGD\n",
        "        num_sgd_iter=30,                 # Number of SGD epochs per training iteration\n",
        "        clip_param=0.2,                  # PPO clipping parameter\n",
        "        vf_clip_param=10.0,              # Value function clipping\n",
        "        entropy_coeff=0.01,              # Entropy regularization coefficient\n",
        "        vf_loss_coeff=1.0,               # Value function loss coefficient\n",
        "        kl_coeff=0.0,                    # KL divergence coefficient\n",
        "        kl_target=0.01,                  # Target KL divergence\n",
        "    )\n",
        "    \n",
        "    # Model architecture\n",
        "    .rl_module(\n",
        "        model_config_dict={\n",
        "            \"fcnet_hiddens\": [256, 256],  # Hidden layer sizes\n",
        "            \"fcnet_activation\": \"relu\",    # Activation function\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    # Rollout configuration\n",
        "    .env_runners(\n",
        "        num_env_runners=4,                # Number of parallel workers\n",
        "        num_envs_per_env_runner=1,       # Environments per worker\n",
        "        rollout_fragment_length=200,      # Steps per rollout fragment\n",
        "    )\n",
        "    \n",
        "    # Evaluation\n",
        "    .evaluation(\n",
        "        evaluation_interval=10,           # Evaluate every N training iterations\n",
        "        evaluation_duration=10,           # Number of episodes for evaluation\n",
        "        evaluation_num_env_runners=1,    # Parallel evaluators\n",
        "    )\n",
        ")\n",
        "\n",
        "# Build and train\n",
        "algo_ppo = config.build_algo()\n",
        "result = algo_ppo.train()\n",
        "print(f\"PPO Training - Episode Reward: {result['env_runners']['episode_return_mean']:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## IMPALA (Importance Weighted Actor-Learner Architecture)\n",
        "\n",
        "IMPALA is an off-policy algorithm designed for distributed training with high throughput.\n",
        "\n",
        "**Key features:**\n",
        "- Off-policy: Can learn from older data\n",
        "- V-trace for importance sampling correction\n",
        "- Designed for massively parallel environments\n",
        "- **Note**: Running in local mode on Windows (distributed training requires Linux/Mac)\n",
        "\n",
        "**Algorithm Overview:**\n",
        "1. Actors collect experience continuously\n",
        "2. Learner trains asynchronously on batches\n",
        "3. V-trace corrects for off-policy data\n",
        "4. Policy updates distributed to actors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ray.rllib.algorithms.impala import IMPALAConfig\n",
        "\n",
        "# Create and configure IMPALA with detailed hyperparameters (Windows-compatible)\n",
        "config = (\n",
        "    IMPALAConfig()\n",
        "    .environment(\"CartPole-v1\")\n",
        "    \n",
        "    # Distributed training configuration (local mode for Windows)\n",
        "    .learners(\n",
        "        num_learners=0,                   # 0 = local mode (no distributed learners)\n",
        "    )\n",
        "    \n",
        "    # Environment runners (data collection workers)\n",
        "    .env_runners(\n",
        "        num_env_runners=4,                # Number of parallel environment workers\n",
        "        num_envs_per_env_runner=1,       # Environments per worker\n",
        "        rollout_fragment_length=50,       # Steps collected per rollout\n",
        "    )\n",
        "    \n",
        "    # Training hyperparameters\n",
        "    .training(\n",
        "        gamma=0.99,                       # Discount factor for future rewards\n",
        "        lr=0.0005,                        # Learning rate for optimizer\n",
        "        train_batch_size=512,             # Batch size for training\n",
        "        grad_clip=40.0,                   # Gradient clipping threshold\n",
        "        grad_clip_by=\"global_norm\",       # Gradient clipping method\n",
        "        entropy_coeff=0.01,               # Entropy regularization (exploration)\n",
        "        vf_loss_coeff=0.5,                # Value function loss coefficient\n",
        "        vtrace_clip_rho_threshold=1.0,    # V-trace importance sampling clip (actor)\n",
        "        vtrace_clip_pg_rho_threshold=1.0, # V-trace importance sampling clip (policy gradient)\n",
        "        num_sgd_iter=1,                   # Number of SGD passes per batch\n",
        "        replay_ratio=0.0,                 # Experience replay ratio\n",
        "        replay_buffer_num_slots=0,        # Replay buffer size (0 = disabled)\n",
        "    )\n",
        "    \n",
        "    # Model architecture\n",
        "    .rl_module(\n",
        "        model_config_dict={\n",
        "            \"fcnet_hiddens\": [256, 256],  # Hidden layer sizes for policy/value networks\n",
        "            \"fcnet_activation\": \"relu\",    # Activation function (relu, tanh, etc.)\n",
        "            \"vf_share_layers\": True,       # Share layers between policy and value function\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    # Evaluation settings\n",
        "    .evaluation(\n",
        "        evaluation_interval=5,            # Evaluate every N training iterations\n",
        "        evaluation_duration=10,           # Number of episodes for evaluation\n",
        "        evaluation_num_env_runners=1,    # Parallel evaluators\n",
        "        evaluation_config={\n",
        "            \"explore\": False,             # Disable exploration during evaluation\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    # Resource allocation\n",
        "    .resources(\n",
        "        num_gpus=0,                       # GPUs for learner (0 = CPU only)\n",
        "        num_cpus_per_env_runner=1,       # CPUs per environment runner\n",
        "    )\n",
        ")\n",
        "\n",
        "# Build and train\n",
        "algo_impala = config.build_algo()\n",
        "result = algo_impala.train()\n",
        "\n",
        "# Display results\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"IMPALA Training Results (Iteration 1)\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Episode Reward Mean:  {result['env_runners']['episode_return_mean']:.2f}\")\n",
        "print(f\"Episode Length Mean:  {result['env_runners']['episode_len_mean']:.2f}\")\n",
        "print(f\"Episodes This Iter:   {result['env_runners']['num_episodes']}\")\n",
        "print(f\"{'='*60}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "def train_algorithm(algo, num_iterations=10, algorithm_name=\"Algorithm\"):\n",
        "    \"\"\"Train an algorithm and track metrics\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    print(f\"\\nTraining {algorithm_name}...\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    for i in range(num_iterations):\n",
        "        result = algo.train()\n",
        "        \n",
        "        episode_reward = result['env_runners']['episode_return_mean']\n",
        "        episode_length = result['env_runners']['episode_len_mean']\n",
        "        \n",
        "        results.append({\n",
        "            'iteration': i + 1,\n",
        "            'reward_mean': episode_reward,\n",
        "            'length_mean': episode_length,\n",
        "        })\n",
        "        \n",
        "        print(f\"Iter {i+1:2d} | Reward: {episode_reward:7.2f} | Length: {episode_length:6.2f}\")\n",
        "    \n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Example usage:\n",
        "# results_ppo = train_algorithm(algo_ppo, num_iterations=20, algorithm_name=\"PPO\")\n",
        "# results_impala = train_algorithm(algo_impala, num_iterations=20, algorithm_name=\"IMPALA\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameter Tuning Guide\n",
        "\n",
        "### If training is unstable:\n",
        "- **Decrease learning rate** (lr): Try 1e-5 to 1e-4\n",
        "- **Increase gradient clipping** (grad_clip): Try 5.0 to 20.0\n",
        "- **Adjust PPO clip_param**: Try 0.1 to 0.3\n",
        "- **Reduce batch size**: Smaller batches = more stable but slower\n",
        "\n",
        "### If learning is too slow:\n",
        "- **Increase learning rate** (lr): Try 1e-3 to 5e-3\n",
        "- **Increase batch size** (train_batch_size): More samples per update\n",
        "- **More env runners**: Collect data faster\n",
        "- **Larger networks**: More capacity to learn complex policies\n",
        "\n",
        "### For better exploration:\n",
        "- **Increase entropy_coeff**: Try 0.01 to 0.1\n",
        "- **Add noise to actions**: Use exploration config\n",
        "- **Decrease gamma**: Focus on short-term rewards\n",
        "- **Use curiosity-driven exploration**: Add intrinsic rewards\n",
        "\n",
        "### For sample efficiency:\n",
        "- **IMPALA with replay**: Set replay_ratio > 0 and replay_buffer_num_slots > 0\n",
        "- **Larger networks**: Increase fcnet_hiddens (e.g., [512, 512])\n",
        "- **More SGD iterations**: Increase num_sgd_iter (PPO only)\n",
        "- **Lower discount factor**: Faster learning on short-term tasks\n",
        "\n",
        "### Common hyperparameter ranges:\n",
        "\n",
        "| Parameter | Typical Range | Good Starting Point | Notes |\n",
        "|-----------|---------------|---------------------|-------|\n",
        "| lr | 1e-5 to 1e-3 | 5e-5 | Lower for continuous control |\n",
        "| gamma | 0.95 to 0.999 | 0.99 | Higher for long-horizon tasks |\n",
        "| entropy_coeff | 0.001 to 0.1 | 0.01 | Higher for more exploration |\n",
        "| train_batch_size | 512 to 8192 | 2048 | Depends on task complexity |\n",
        "| fcnet_hiddens | [64,64] to [512,512] | [256,256] | Larger for complex tasks |\n",
        "| clip_param (PPO) | 0.1 to 0.3 | 0.2 | Standard value works well |\n",
        "| num_sgd_iter (PPO) | 3 to 30 | 10-20 | More = better sample efficiency |\n",
        "\n",
        "### Algorithm Selection Guide:\n",
        "\n",
        "**Use PPO when:**\n",
        "- You want stable, reliable training\n",
        "- Sample efficiency is important\n",
        "- You have moderate compute resources\n",
        "- Task has sparse rewards\n",
        "\n",
        "**Use IMPALA when:**\n",
        "- You need high throughput\n",
        "- You have many parallel environments\n",
        "- Wall-clock time is more important than sample efficiency\n",
        "- You have distributed compute resources\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
